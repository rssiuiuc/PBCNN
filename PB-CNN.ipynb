{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#import kerastuner as kt\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "dist = tfp.distributions\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, Reshape, Dense, multiply, Permute, Concatenate, Conv2D, Add, Activation, Lambda\n",
    "from keras import backend as K\n",
    "from keras.activations import sigmoid\n",
    "from keras.layers import Input\n",
    "\n",
    "import tensorflow_probability as tfp\n",
    "dist = tfp.distributions\n",
    "\n",
    "# define the gaussion distribution for the output\n",
    "tfd = tfp.distributions\n",
    "def normal_sp(params):\n",
    "  return tfd.Normal(loc=params[:,0:1], scale=1e-3 + tf.math.softplus(0.05 * params[:,1:2]))# both parameters are learnable\n",
    "\n",
    "# define the loss function,which stands for Negative Log-Likelihood. This function takes two arguments: `y` and `distr`.\n",
    "'''\n",
    "The `y` argument represents the observed data. This could be any form of data that you're trying to model, such as the heights of a group of people, the scores of a group of students, etc.\n",
    "\n",
    "The `distr` argument represents a probability distribution. In the context of this function, it's expected that `distr` is an object that has a method called `log_prob`. This method should take a data point (like `y`) and return the natural logarithm of the probability of that data point according to the distribution.\n",
    "\n",
    "The function `NLL` calculates the negative log-likelihood of the observed data `y` given the probability distribution `distr`. In other words, it calculates how likely the observed data is if we assume that it follows the given distribution. \n",
    "\n",
    "The negative sign is used because likelihoods are usually maximized, while in optimization problems (like training a machine learning model), we usually minimize. So, by taking the negative log-likelihood, we can minimize this value to get the best parameters for our model.\n",
    "\n",
    "A lower negative log-likelihood means that the model fits the data better.\n",
    "'''\n",
    "def NLL(y, distr):\n",
    "  return -distr.log_prob(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "start_year=2008\n",
    "\n",
    "# all variables except soil (samples,11,6,16)\n",
    "train_phen=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/train_phen'+str(start_year)+'.npy')\n",
    "test_phen=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/test_phen'+str(start_year)+'.npy')\n",
    "\n",
    "# soil variables (samples,11,6,36)\n",
    "train_soil=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/train_soil'+str(start_year)+'.npy')\n",
    "test_soil=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/test_soil'+str(start_year)+'.npy')\n",
    "\n",
    "train_y=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/train_y'+str(start_year)+'.npy')\n",
    "test_y=np.load('/content/drive/MyDrive/Yield_paper_results3/Input/test_y'+str(start_year)+'.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seed value, both for numpy and tensorflow\n",
    "seed_value= 2022\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "keras.backend.clear_session()\n",
    "\n",
    "# define the model, MC-dropout is activated by setting training=True\n",
    "# set the input shape\n",
    "input_all = keras.Input(shape=(11, 6, 4+5+2+5))\n",
    "inputA = layers.Lambda(lambda x: x[:,:,:,0:4])(input_all)# 0:4 means EVI2 group\n",
    "inputB = layers.Lambda(lambda x: x[:,:,:,4:4+4])(input_all)# 4:4+4 means the Heat-related group\n",
    "inputC = layers.Lambda(lambda x: x[:,:,:,8:])(input_all) # 8: means the Water-related group\n",
    "input_soil=keras.Input(shape=(11, 6, 6*6))# soil group\n",
    "\n",
    "x1=layers.Conv2D(4, kernel_size=(1,1), strides=(1,1),activation=tf.nn.relu)(inputA)\n",
    "x1=layers.BatchNormalization()(x1)\n",
    "x1=layers.Dropout(0.25)(x1, training=True) # training=True means the dropout is activated during both training and testing time\n",
    "\n",
    "x2=layers.Conv2D(4, kernel_size=(1,1), strides=(1,1), activation=tf.nn.relu)(inputB)\n",
    "x2=layers.BatchNormalization()(x2)\n",
    "x2=layers.Dropout(0.5)(x2, training=True)\n",
    "\n",
    "x3=layers.Conv2D(8, kernel_size=(1,1), strides=(1,1), activation=tf.nn.relu)(inputC)\n",
    "x3=layers.BatchNormalization()(x3)\n",
    "x3=layers.Dropout(0.5)(x3, training=True)\n",
    "\n",
    "x4=layers.Conv2D(8, kernel_size=(1,1), strides=(1,1), activation=tf.nn.relu)(input_soil)\n",
    "x4=layers.BatchNormalization()(x4)\n",
    "x4=layers.Dropout(0.5)(x4, training=True)\n",
    "\n",
    "merged = layers.Concatenate()([x1,x2,x3,x4])\n",
    "z=layers.Dropout(0.5)(merged, training=True)\n",
    "\n",
    "z=layers.Conv2D(32, kernel_size=(3,3), strides=(1,1),padding='same',\n",
    "                            activation=tf.nn.relu)(z)\n",
    "z=layers.BatchNormalization()(z)\n",
    "z=layers.Dropout(0.25)(z, training=True)\n",
    "\n",
    "z=layers.Conv2D(16, kernel_size=(3,3), strides=(1,1),padding='same',\n",
    "activation=tf.nn.relu)(z)\n",
    "z=layers.BatchNormalization()(z)\n",
    "z=layers.Dropout(0.25)(z, training=True)\n",
    "\n",
    "z=layers.Flatten()(z)\n",
    "\n",
    "z=layers.Dense(1024)(z)\n",
    "z=layers.Dropout(0.1)(z, training=True)\n",
    "\n",
    "params = layers.Dense(2)(z)\n",
    "outputs = tfp.layers.DistributionLambda(normal_sp)(params)# the output is a distribution\n",
    "\n",
    "model=tf.keras.Model(inputs=[input_all,input_soil], outputs=outputs)\n",
    "\n",
    "# compile model\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=opt,loss=NLL,metrics=['RootMeanSquaredError']) # the loss function is NLL\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
    "\n",
    "# fit model\n",
    "history = model.fit([train_phen,train_soil], train_y, epochs=1000, batch_size=32, verbose=0, validation_split = 0.2, callbacks=[es])\n",
    "\n",
    "# retrain the model with a smaller learning rate\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=opt,loss=NLL,metrics=['RootMeanSquaredError'])\n",
    "\n",
    "# fit model\n",
    "history = model.fit([train_phen,train_soil], train_y, epochs=300, batch_size=32, verbose=0, validation_split = 0.2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input=[test_phen,test_soil]\n",
    "# predict the mean and std of the output\n",
    "predicted = []\n",
    "predicted_stds = []\n",
    "predicted_means = []\n",
    "\n",
    "for _ in range(10000):\n",
    "    predicted.append(model.predict([test_input]))\n",
    "\n",
    "    prediction_distribution = model(test_input)\n",
    "    prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
    "    prediction_stdv = prediction_distribution.stddev().numpy()\n",
    "\n",
    "    predicted_means.append(prediction_mean)\n",
    "    predicted_stds.append(prediction_stdv)\n",
    "\n",
    "mu_pred =np.mean(predicted,axis=0)\n",
    "print('rmse',np.sqrt(sum((mu_pred.reshape([test_y.shape[0],])-test_y)**2)/test_y.shape[0]))\n",
    "\n",
    "sigma_pred =np.std(predicted,axis=0) # the total uncertainty\n",
    "print('averaged total uncertainty',np.mean(sigma_pred))\n",
    "\n",
    "sigma_model =np.std(predicted_means,axis=0) # the epistemic uncertainty\n",
    "sigma_data =np.mean(predicted_stds,axis=0) # the aleatoric uncertainty\n",
    "\n",
    "print('averaged epistemic uncertainty',np.mean(sigma_model))\n",
    "print('averaged aleatoric uncertainty',np.mean(sigma_data))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
